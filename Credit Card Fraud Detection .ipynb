{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igAvinashSingh/ML-ASSIGNMENT2/blob/main/Credit%20Card%20Fraud%20Detection%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8NeL6TssIUI",
        "outputId": "b1fa653f-27c8-47c2-e91e-aa70042542c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GPU-optimized credit card fraud detection pipeline...\n",
            "Checking for GPU availability...\n",
            "GPU is available!\n",
            "Starting Data Loading...\n",
            "Local file not found, creating synthetic dataset for testing...\n",
            "Synthetic data created with shape: (10000, 31)\n",
            "Fraud transactions: 20.0\n",
            "‚è± Data Loading: 0.02 seconds\n",
            "Dataset shape: (10000, 31)\n",
            "Number of fraud cases: 20.0\n",
            "Percentage of fraud: 0.2000%\n",
            "Starting Enhanced EDA...\n",
            "Performing enhanced exploratory data analysis...\n",
            "‚è± Enhanced EDA: 5.91 seconds\n",
            "Starting Data Preprocessing...\n",
            "‚è± Data Preprocessing: 4.26 seconds\n",
            "Starting Train-Test Split...\n",
            "Training set: (8000, 31), Test set: (2000, 31)\n",
            "Fraud cases in training: 16.0, test: 4.0\n",
            "‚è± Train-Test Split: 0.44 seconds\n",
            "Starting SMOTE Resampling...\n",
            "Applying SMOTE with 10% sampling ratio...\n",
            "After SMOTE - X shape: (8782, 31), Fraud cases: 798.0\n",
            "SMOTE failed: slice indices must be integers or None or have an __index__ method\n",
            "Attempting basic random oversampling as fallback...\n",
            "After manual oversampling - X shape: (8782, 31), Fraud cases: 798.0\n",
            "‚è± SMOTE Resampling: 0.58 seconds\n",
            "Starting Model Training and Evaluation...\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.9935\n",
            "Precision: 0.2353\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.3810\n",
            "AUC: 1.0000\n",
            "Training Time: 0.05 seconds\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "Accuracy: 0.9980\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUC: 0.9952\n",
            "Training Time: 2.45 seconds\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost Results:\n",
            "Accuracy: 0.9990\n",
            "Precision: 1.0000\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.6667\n",
            "AUC: 1.0000\n",
            "Training Time: 1.01 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
            "Neural Network Results:\n",
            "Accuracy: 0.8680\n",
            "Precision: 0.0149\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.0294\n",
            "AUC: 0.9984\n",
            "Training Time: 14.52 seconds\n",
            "\n",
            "Training Support Vector Machine...\n",
            "Support Vector Machine Results:\n",
            "Accuracy: 0.9990\n",
            "Precision: 1.0000\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.6667\n",
            "AUC: 1.0000\n",
            "Training Time: 0.52 seconds\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Results:\n",
            "Accuracy: 0.9990\n",
            "Precision: 1.0000\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.6667\n",
            "AUC: 0.7501\n",
            "Training Time: 12.30 seconds\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "Accuracy: 0.9975\n",
            "Precision: 0.3333\n",
            "Recall: 0.2500\n",
            "F1 Score: 0.2857\n",
            "AUC: 0.6245\n",
            "Training Time: 0.06 seconds\n",
            "\n",
            "‚è± Model Training and Evaluation: 37.30 seconds\n",
            "\n",
            "üèÜ Best performing model: XGBoost\n",
            "                 Model  Accuracy  Precision  Recall  F1 Score      AUC  Training Time\n",
            "               XGBoost    0.9990   1.000000    0.50  0.666667 1.000000       1.008859\n",
            "     Gradient Boosting    0.9990   1.000000    0.50  0.666667 0.750125      12.295430\n",
            "Support Vector Machine    0.9990   1.000000    0.50  0.666667 1.000000       0.523070\n",
            "   Logistic Regression    0.9935   0.235294    1.00  0.380952 1.000000       0.045486\n",
            "         Decision Tree    0.9975   0.333333    0.25  0.285714 0.624499       0.062398\n",
            "        Neural Network    0.8680   0.014925    1.00  0.029412 0.998372      14.516525\n",
            "         Random Forest    0.9980   0.000000    0.00  0.000000 0.995240       2.452104\n",
            "Starting Feature Importance Analysis...\n",
            "‚è± Feature Importance Analysis: 0.88 seconds\n",
            "\n",
            "Top 10 most important features:\n",
            "  Feature  Importance\n",
            "0      V3    0.564534\n",
            "1      V2    0.279639\n",
            "2      V1    0.114763\n",
            "3     V12    0.012423\n",
            "4      V9    0.003271\n",
            "5      V6    0.003104\n",
            "6      V5    0.002710\n",
            "7     V26    0.002359\n",
            "8      V8    0.002141\n",
            "9     V14    0.001907\n",
            "Starting Fraud Detection Simulation...\n",
            "Simulating real-time fraud detection pipeline...\n",
            "Simulation Results:\n",
            "Accuracy: 77.78%\n",
            "Average Detection Time: 2.28 ms\n",
            "‚è± Fraud Detection Simulation: 3.13 seconds\n",
            "Starting Performance Summary...\n",
            "\n",
            "================================================================================\n",
            "CREDIT CARD FRAUD DETECTION - PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Total Processing Time: 52.55 seconds\n",
            "\n",
            "Dataset Summary:\n",
            "- Total transactions: 10000\n",
            "- Fraud transactions: 20 (0.2000%)\n",
            "- Normal transactions: 9980\n",
            "\n",
            "Pipeline Performance:\n",
            "- Data Loading: 0.02 seconds\n",
            "- Enhanced EDA: 5.91 seconds\n",
            "- Data Preprocessing: 4.26 seconds\n",
            "- Train-Test Split: 0.44 seconds\n",
            "- SMOTE Resampling: 0.58 seconds\n",
            "- Model Training and Evaluation: 37.30 seconds\n",
            "- Feature Importance Analysis: 0.88 seconds\n",
            "- Fraud Detection Simulation: 3.13 seconds\n",
            "\n",
            "Best Model: XGBoost\n",
            "- Accuracy: 0.9990\n",
            "- Precision: 1.0000\n",
            "- Recall: 0.5000\n",
            "- F1 Score: 0.6667\n",
            "- AUC: 1.0000\n",
            "\n",
            "Top 5 Most Important Features:\n",
            "- V3: 0.5645\n",
            "- V2: 0.2796\n",
            "- V1: 0.1148\n",
            "- V12: 0.0124\n",
            "- V9: 0.0033\n",
            "\n",
            "Performance summary saved to performance_summary.txt\n",
            "\n",
            "All visualizations saved to the 'plots' directory.\n",
            "‚è± Performance Summary: 0.00 seconds\n",
            "\n",
            "‚úÖ Credit card fraud detection pipeline complete!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies with correct versions\n",
        "!pip install imbalanced-learn xgboost scikit-learn matplotlib seaborn tensorflow --quiet\n",
        "\n",
        "# Import necessary libraries\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Starting GPU-optimized credit card fraud detection pipeline...\")\n",
        "\n",
        "# Configure matplotlib to display properly in the current environment\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "except:\n",
        "    plt.switch_backend('agg')\n",
        "\n",
        "# Use a modern and appealing style for plots\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score\n",
        "from sklearn.metrics import recall_score, accuracy_score, f1_score, roc_curve, auc, precision_recall_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Create custom colormaps for better visualizations\n",
        "fraud_cmap = LinearSegmentedColormap.from_list('fraud_cmap', ['#f5f7fa', '#c3101c'])\n",
        "blue_cmap = LinearSegmentedColormap.from_list('blue_cmap', ['#f5f7fa', '#1a53a2'])\n",
        "\n",
        "# Check for GPU availability and optimize\n",
        "print(\"Checking for GPU availability...\")\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available!\")\n",
        "    # Set memory growth to avoid OOM errors\n",
        "    for gpu in tf.config.list_physical_devices('GPU'):\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    # Set XGBoost to use GPU\n",
        "    gpu_params = {'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'gpu_id': 0}\n",
        "else:\n",
        "    print(\"No GPU found, using CPU.\")\n",
        "    gpu_params = {}\n",
        "\n",
        "# Performance tracking setup\n",
        "perf_metrics = {}\n",
        "start_time_total = time.time()\n",
        "\n",
        "# Function to measure execution time\n",
        "def time_operation(operation_name):\n",
        "    def decorator(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"Starting {operation_name}...\")\n",
        "            start_time = time.time()\n",
        "            result = func(*args, **kwargs)\n",
        "            end_time = time.time()\n",
        "            duration = end_time - start_time\n",
        "            perf_metrics[operation_name] = duration\n",
        "            print(f\"‚è± {operation_name}: {duration:.2f} seconds\")\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Create a directory for saving plots\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "# Step 1: Load dataset with error handling\n",
        "@time_operation(\"Data Loading\")\n",
        "def load_dataset():\n",
        "    # Check if file exists locally first\n",
        "    file_path = \"creditcard.csv\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"Loading dataset from local file: {file_path}\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"Local file not found, creating synthetic dataset for testing...\")\n",
        "        return create_synthetic_data()\n",
        "\n",
        "def create_synthetic_data():\n",
        "    \"\"\"Create a small synthetic dataset for testing when download fails\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_samples = 10000\n",
        "\n",
        "    # Generate features (V1-V28, Time, Amount)\n",
        "    X = np.random.randn(n_samples, 28)  # Generate exactly 28 V columns\n",
        "    time = np.random.randint(0, 172800, size=n_samples)\n",
        "    amount = np.random.exponential(scale=100, size=n_samples)\n",
        "\n",
        "    # Make some features more discriminative for fraud detection\n",
        "    # Generate target (mostly 0s, few 1s for fraud)\n",
        "    y = np.zeros(n_samples)\n",
        "    fraud_idx = np.random.choice(n_samples, size=int(n_samples * 0.002), replace=False)\n",
        "    y[fraud_idx] = 1\n",
        "\n",
        "    # Make V1, V2, V3 more discriminative for fraud detection\n",
        "    X[fraud_idx, 0] = np.random.normal(-3, 1, size=len(fraud_idx))  # V1 for fraud\n",
        "    X[fraud_idx, 1] = np.random.normal(2, 1, size=len(fraud_idx))   # V2 for fraud\n",
        "    X[fraud_idx, 2] = np.random.normal(-4, 1.5, size=len(fraud_idx))  # V3 for fraud\n",
        "\n",
        "    # Create DataFrame - fixed column numbering to match expected format\n",
        "    df = pd.DataFrame(X, columns=[f'V{i}' for i in range(1, 29)])\n",
        "    df['Time'] = time\n",
        "    df['Amount'] = amount\n",
        "    df['Class'] = y\n",
        "\n",
        "    print(\"Synthetic data created with shape:\", df.shape)\n",
        "    print(\"Fraud transactions:\", df['Class'].sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "df = load_dataset()\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of fraud cases: {df['Class'].sum()}\")\n",
        "print(f\"Percentage of fraud: {100 * df['Class'].mean():.4f}%\")\n",
        "\n",
        "# Step 2: Enhanced Exploratory Data Analysis\n",
        "@time_operation(\"Enhanced EDA\")\n",
        "def enhanced_eda(dataframe):\n",
        "    print(\"Performing enhanced exploratory data analysis...\")\n",
        "\n",
        "    # Create a figure for class distribution with count and percentage\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1])\n",
        "\n",
        "    # Plot 1: Class distribution bar plot\n",
        "    ax1 = plt.subplot(gs[0])\n",
        "    class_counts = dataframe['Class'].value_counts()\n",
        "    sns.barplot(x=class_counts.index, y=class_counts.values, ax=ax1, palette=['#3498db', '#e74c3c'])\n",
        "\n",
        "    for i, count in enumerate(class_counts.values):\n",
        "        ax1.text(i, count + 50, f\"{count:,}\", ha='center', fontweight='bold')\n",
        "        percentage = 100 * count / len(dataframe)\n",
        "        ax1.text(i, count//2, f\"{percentage:.2f}%\", ha='center', color='white', fontweight='bold')\n",
        "\n",
        "    ax1.set_title('Transaction Class Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Class (0: Normal, 1: Fraud)', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "    ax1.set_xticklabels(['Normal', 'Fraud'])\n",
        "    ax1.grid(False)\n",
        "\n",
        "    # Plot 2: Pie chart\n",
        "    ax2 = plt.subplot(gs[1])\n",
        "    labels = ['Normal', 'Fraud']\n",
        "    ax2.pie(class_counts.values, labels=labels, autopct='%1.2f%%',\n",
        "            colors=['#3498db', '#e74c3c'], startangle=90, explode=(0, 0.1),\n",
        "            textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
        "    ax2.set_title('Class Percentage', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Transaction amount analysis: Normal vs Fraud\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    gs = gridspec.GridSpec(1, 2)\n",
        "\n",
        "    # Plot 1: Amount distribution by class\n",
        "    ax1 = plt.subplot(gs[0])\n",
        "    normal = dataframe[dataframe['Class'] == 0]['Amount']\n",
        "    fraud = dataframe[dataframe['Class'] == 1]['Amount']\n",
        "\n",
        "    # Use log scale for better visualization\n",
        "    sns.histplot(normal, color='#3498db', alpha=0.5, label='Normal',\n",
        "                bins=50, kde=True, log_scale=(False, True), ax=ax1)\n",
        "    sns.histplot(fraud, color='#e74c3c', alpha=0.7, label='Fraud',\n",
        "                bins=50, kde=True, log_scale=(False, True), ax=ax1)\n",
        "\n",
        "    ax1.set_title('Transaction Amount Distribution by Class', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Amount', fontsize=12)\n",
        "    ax1.set_ylabel('Frequency (log scale)', fontsize=12)\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot 2: Box plot comparison\n",
        "    ax2 = plt.subplot(gs[1])\n",
        "    sns.boxplot(x='Class', y='Amount', data=dataframe, ax=ax2,\n",
        "               palette=['#3498db', '#e74c3c'])\n",
        "\n",
        "    # Add statistics\n",
        "    for i, cls in enumerate([0, 1]):\n",
        "        subset = dataframe[dataframe['Class'] == cls]['Amount']\n",
        "        ax2.text(i, subset.median() + 10, f\"Median: ${subset.median():.2f}\",\n",
        "                ha='center', fontweight='bold')\n",
        "        ax2.text(i, subset.mean(), f\"Mean: ${subset.mean():.2f}\",\n",
        "                ha='center', fontweight='bold', color='black',\n",
        "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
        "\n",
        "    ax2.set_title('Amount Boxplot by Class', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Class (0: Normal, 1: Fraud)', fontsize=12)\n",
        "    ax2.set_ylabel('Amount ($)', fontsize=12)\n",
        "    ax2.set_xticklabels(['Normal', 'Fraud'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/amount_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Time analysis\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    # Convert time to hours for better interpretation\n",
        "    dataframe['Time_hr'] = dataframe['Time'] / 3600\n",
        "\n",
        "    ax = sns.histplot(data=dataframe, x='Time_hr', hue='Class',\n",
        "                     palette=['#3498db', '#e74c3c'], bins=48,\n",
        "                     multiple='stack', alpha=0.7)\n",
        "\n",
        "    ax.set_title('Transaction Count by Hour of Day', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Time (hours)', fontsize=12)\n",
        "    ax.set_ylabel('Transaction Count', fontsize=12)\n",
        "    ax.legend(['Normal', 'Fraud'])\n",
        "\n",
        "    # Add grid lines for every 6 hours\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(6))\n",
        "    ax.grid(which='major', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/time_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance analysis using correlation\n",
        "    plt.figure(figsize=(18, 10))\n",
        "    corr_with_class = dataframe.corr()['Class'].sort_values(ascending=False)\n",
        "\n",
        "    # Get top correlated features (positive and negative)\n",
        "    top_corr = pd.concat([corr_with_class.head(15), corr_with_class.tail(15)])\n",
        "    top_corr = top_corr[top_corr.index != 'Class']  # Remove the class itself\n",
        "\n",
        "    colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_corr.values]\n",
        "    sns.barplot(x=top_corr.values, y=top_corr.index, palette=colors)\n",
        "\n",
        "    plt.title('Top Features Correlated with Fraud', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Correlation Coefficient', fontsize=14)\n",
        "    plt.axvline(x=0, color='black', linestyle='--')\n",
        "    plt.grid(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/feature_correlation.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # PCA visualization for 2D representation\n",
        "    # Apply PCA\n",
        "    X = dataframe.drop('Class', axis=1)\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Create DataFrame with PCA results\n",
        "    pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "    pca_df['Class'] = dataframe['Class'].values\n",
        "\n",
        "    # Plot PCA with improved styling\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # First plot all normal transactions with low alpha\n",
        "    normal = pca_df[pca_df['Class'] == 0]\n",
        "    fraud = pca_df[pca_df['Class'] == 1]\n",
        "\n",
        "    # Plot normal transactions (sample to reduce plotting time)\n",
        "    normal_sample = normal.sample(min(5000, len(normal)), random_state=42)\n",
        "    plt.scatter(normal_sample['PC1'], normal_sample['PC2'],\n",
        "               c='#3498db', label='Normal', alpha=0.5, s=10)\n",
        "\n",
        "    # Plot all fraud transactions\n",
        "    plt.scatter(fraud['PC1'], fraud['PC2'],\n",
        "               c='#e74c3c', label='Fraud', alpha=0.9, s=30, edgecolor='black')\n",
        "\n",
        "    plt.title('PCA: Normal vs Fraud Transactions', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel(f'Principal Component 1 (Variance: {pca.explained_variance_ratio_[0]:.2%})', fontsize=14)\n",
        "    plt.ylabel(f'Principal Component 2 (Variance: {pca.explained_variance_ratio_[1]:.2%})', fontsize=14)\n",
        "    plt.legend(title='Transaction Type', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add annotation showing total variance explained\n",
        "    total_var = sum(pca.explained_variance_ratio_)\n",
        "    plt.annotate(f'Total Variance Explained: {total_var:.2%}',\n",
        "                xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"#f8f9fa\", ec=\"gray\", alpha=0.8),\n",
        "                fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/pca_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Return a few statistics for later use\n",
        "    stats = {\n",
        "        'fraud_rate': 100 * dataframe['Class'].mean(),\n",
        "        'normal_mean_amount': dataframe[dataframe['Class'] == 0]['Amount'].mean(),\n",
        "        'fraud_mean_amount': dataframe[dataframe['Class'] == 1]['Amount'].mean(),\n",
        "        'top_correlated_features': corr_with_class.head(5).index.tolist()\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Run enhanced EDA\n",
        "eda_stats = enhanced_eda(df)\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "@time_operation(\"Data Preprocessing\")\n",
        "def preprocess_data(dataframe):\n",
        "    # Handle missing values if any\n",
        "    if dataframe.isnull().sum().sum() > 0:\n",
        "        print(f\"Found {dataframe.isnull().sum().sum()} missing values, handling them...\")\n",
        "        dataframe.fillna(dataframe.mean(), inplace=True)\n",
        "\n",
        "    # Create a copy of the original dataframe for visualization\n",
        "    df_processed = dataframe.copy()\n",
        "\n",
        "    # Extract features and target\n",
        "    X = dataframe.drop(columns=['Class'])\n",
        "    y = dataframe['Class']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Visualize the scaling effect on a few important features\n",
        "    if 'V1' in dataframe.columns and 'V2' in dataframe.columns and 'V3' in dataframe.columns:\n",
        "        # Pick a few important features for visualization\n",
        "        features_to_plot = ['Amount', 'V1', 'V2', 'V3', 'V4']\n",
        "\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        for i, feature in enumerate(features_to_plot):\n",
        "            if feature in dataframe.columns:\n",
        "                # Original distribution\n",
        "                plt.subplot(len(features_to_plot), 2, 2*i+1)\n",
        "                sns.histplot(dataframe[feature][dataframe['Class']==0], color='blue',\n",
        "                           label='Normal', alpha=0.5, kde=True)\n",
        "                sns.histplot(dataframe[feature][dataframe['Class']==1], color='red',\n",
        "                           label='Fraud', alpha=0.5, kde=True)\n",
        "                plt.title(f'Original {feature} Distribution')\n",
        "                plt.legend()\n",
        "\n",
        "                # Scaled distribution\n",
        "                plt.subplot(len(features_to_plot), 2, 2*i+2)\n",
        "                feature_idx = list(X.columns).index(feature)\n",
        "                sns.histplot(X_scaled[:, feature_idx][y==0], color='blue',\n",
        "                           label='Normal', alpha=0.5, kde=True)\n",
        "                sns.histplot(X_scaled[:, feature_idx][y==1], color='red',\n",
        "                           label='Fraud', alpha=0.5, kde=True)\n",
        "                plt.title(f'Scaled {feature} Distribution')\n",
        "                plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('plots/feature_scaling_effect.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    return X_scaled, y.values, df_processed\n",
        "\n",
        "X_scaled_np, y_np, df_processed = preprocess_data(df)\n",
        "\n",
        "# Train-Test Split with stratification\n",
        "@time_operation(\"Train-Test Split\")\n",
        "def split_data(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "    print(f\"Fraud cases in training: {np.sum(y_train)}, test: {np.sum(y_test)}\")\n",
        "\n",
        "    # Visualize the train-test split\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot 1: Class distribution in train set\n",
        "    plt.subplot(1, 2, 1)\n",
        "    train_counts = pd.Series(y_train).value_counts()\n",
        "    train_labels = ['Normal', 'Fraud'] if len(train_counts) > 1 else ['Normal']\n",
        "    plt.pie(train_counts, labels=train_labels, autopct='%1.2f%%',\n",
        "           colors=['#3498db', '#e74c3c'], explode=[0, 0.1] if len(train_counts) > 1 else [0])\n",
        "    plt.title('Training Set Class Distribution')\n",
        "\n",
        "    # Plot 2: Class distribution in test set\n",
        "    plt.subplot(1, 2, 2)\n",
        "    test_counts = pd.Series(y_test).value_counts()\n",
        "    test_labels = ['Normal', 'Fraud'] if len(test_counts) > 1 else ['Normal']\n",
        "    plt.pie(test_counts, labels=test_labels, autopct='%1.2f%%',\n",
        "           colors=['#3498db', '#e74c3c'], explode=[0, 0.1] if len(test_counts) > 1 else [0])\n",
        "    plt.title('Test Set Class Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/train_test_split.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_data(X_scaled_np, y_np)\n",
        "\n",
        "# Apply SMOTE with GPU acceleration if available\n",
        "@time_operation(\"SMOTE Resampling\")\n",
        "def apply_smote(X_train, y_train):\n",
        "    try:\n",
        "        print(\"Applying SMOTE with 10% sampling ratio...\")\n",
        "        smote = SMOTE(random_state=42, sampling_strategy=0.1)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "        print(f\"After SMOTE - X shape: {X_resampled.shape}, Fraud cases: {np.sum(y_resampled)}\")\n",
        "\n",
        "        # Visualize before and after SMOTE\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Plot 1: Before SMOTE\n",
        "        plt.subplot(1, 2, 1)\n",
        "        before_counts = pd.Series(y_train).value_counts()\n",
        "        plt.bar([0, 1], before_counts, color=['#3498db', '#e74c3c'])\n",
        "        for i, count in enumerate(before_counts):\n",
        "            plt.text(i, count//2, f\"{count}\\n({100*count/len(y_train):.2f}%)\",\n",
        "                   ha='center', color='white', fontweight='bold')\n",
        "        plt.title('Class Distribution Before SMOTE')\n",
        "        plt.xticks([0, 1], ['Normal', 'Fraud'])\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # Plot 2: After SMOTE\n",
        "        plt.subplot(1, 2, 2)\n",
        "        after_counts = pd.Series(y_resampled).value_counts()\n",
        "        plt.bar([0, 1], after_counts, color=['#3498db', '#e74c3c'])\n",
        "        for i, count in enumerate(after_counts):\n",
        "            plt.text(i, count//2, f\"{count}\\n({100*count/len(y_resampled):.2f}%)\",\n",
        "                   ha='center', color='white', fontweight='bold')\n",
        "        plt.title('Class Distribution After SMOTE')\n",
        "        plt.xticks([0, 1], ['Normal', 'Fraud'])\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('plots/smote_effect.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Visualize SMOTE effect with PCA\n",
        "        all_data = np.vstack((X_train, X_resampled[np.sum(y_train):]))\n",
        "        all_labels = np.hstack((y_train, y_resampled[np.sum(y_train):]))\n",
        "\n",
        "        # Flag original vs synthetic samples\n",
        "        origin = np.hstack((np.zeros(len(X_train)), np.ones(len(X_resampled) - len(X_train))))\n",
        "\n",
        "        # Apply PCA for visualization\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(all_data)\n",
        "\n",
        "        # Create a dataframe for plotting\n",
        "        pca_df = pd.DataFrame({\n",
        "            'PC1': pca_result[:, 0],\n",
        "            'PC2': pca_result[:, 1],\n",
        "            'Class': all_labels,\n",
        "            'Origin': ['Original' if o == 0 else 'Synthetic' for o in origin]\n",
        "        })\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot normal samples\n",
        "        plt.scatter(\n",
        "            pca_df[pca_df['Class'] == 0]['PC1'],\n",
        "            pca_df[pca_df['Class'] == 0]['PC2'],\n",
        "            c='#3498db', alpha=0.5, s=10, label='Normal'\n",
        "        )\n",
        "\n",
        "        # Plot original fraud samples\n",
        "        plt.scatter(\n",
        "            pca_df[(pca_df['Class'] == 1) & (pca_df['Origin'] == 'Original')]['PC1'],\n",
        "            pca_df[(pca_df['Class'] == 1) & (pca_df['Origin'] == 'Original')]['PC2'],\n",
        "            c='#e74c3c', s=30, label='Original Fraud', alpha=0.9\n",
        "        )\n",
        "\n",
        "        # Plot synthetic fraud samples\n",
        "        plt.scatter(\n",
        "            pca_df[(pca_df['Class'] == 1) & (pca_df['Origin'] == 'Synthetic')]['PC1'],\n",
        "            pca_df[(pca_df['Class'] == 1) & (pca_df['Origin'] == 'Synthetic')]['PC2'],\n",
        "            c='#2ecc71', s=20, label='Synthetic Fraud', alpha=0.7\n",
        "        )\n",
        "\n",
        "        plt.title('PCA: Original vs. SMOTE-generated Samples', fontsize=16, fontweight='bold')\n",
        "        plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
        "        plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
        "        plt.legend(title='Sample Type', fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('plots/smote_pca_visualization.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SMOTE failed: {e}\")\n",
        "        print(\"Attempting basic random oversampling as fallback...\")\n",
        "\n",
        "        # Simple random oversampling as fallback\n",
        "        fraud_indices = np.where(y_train == 1)[0]\n",
        "        non_fraud_indices = np.where(y_train == 0)[0]\n",
        "\n",
        "        # Oversample fraud cases to 10% of non-fraud\n",
        "        target_samples = int(len(non_fraud_indices) * 0.1)\n",
        "        oversample_indices = np.random.choice(\n",
        "            fraud_indices,\n",
        "            size=max(target_samples - len(fraud_indices), 0),\n",
        "            replace=True\n",
        "        )\n",
        "\n",
        "        # Combine with original fraud indices\n",
        "        all_fraud_indices = np.concatenate([fraud_indices, oversample_indices])\n",
        "        combined_indices = np.concatenate([non_fraud_indices, all_fraud_indices])\n",
        "\n",
        "        # Shuffle the indices\n",
        "        np.random.shuffle(combined_indices)\n",
        "\n",
        "        X_resampled = X_train[combined_indices]\n",
        "        y_resampled = y_train[combined_indices]\n",
        "\n",
        "        print(f\"After manual oversampling - X shape: {X_resampled.shape}, Fraud cases: {np.sum(y_resampled)}\")\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "X_train_resampled, y_train_resampled = apply_smote(X_train, y_train)\n",
        "\n",
        "# Create TensorFlow model optimized for GPU\n",
        "def create_tensorflow_model(input_dim):\n",
        "    # Reduce batch size for T4 GPU memory constraints\n",
        "    # Use mixed precision for faster training\n",
        "    try:\n",
        "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    except:\n",
        "        print(\"Mixed precision not supported, using default precision\")\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile with appropriate optimizer for GPU\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the models dictionary scope at the global level\n",
        "models = {}\n",
        "\n",
        "# Step 4: Train GPU-optimized models\n",
        "@time_operation(\"Model Training and Evaluation\")\n",
        "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
        "    # Configure models optimized for GPU/performance\n",
        "    global models\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(\n",
        "            max_iter=300,\n",
        "            solver='liblinear',\n",
        "            C=0.1,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        \"Random Forest\": RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            min_samples_split=10,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        \"XGBoost\": XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            eval_metric=\"logloss\",\n",
        "            **gpu_params\n",
        "        ),\n",
        "        \"Neural Network\": create_tensorflow_model(X_train.shape[1]),\n",
        "        \"Support Vector Machine\": SVC(\n",
        "            kernel='rbf',\n",
        "            C=1.0,\n",
        "            gamma='scale',\n",
        "            probability=True,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        ),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(\n",
        "            max_depth=8,\n",
        "            min_samples_split=10,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        )\n",
        "    }\n",
        "\n",
        "    performance = []\n",
        "    best_model = None\n",
        "    best_f1 = -1\n",
        "\n",
        "    # Store training history for Neural Network\n",
        "    nn_history = None\n",
        "\n",
        "    # Store predictions for later visualization\n",
        "    all_predictions = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model_start = time.time()\n",
        "\n",
        "        print(f\"Training {name}...\")\n",
        "        try:\n",
        "            if name == \"Neural Network\":\n",
        "                # TensorFlow model requires different training approach\n",
        "                history = model.fit(\n",
        "                    X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=10,\n",
        "                    batch_size=2048,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Store history for later visualization\n",
        "                nn_history = history.history\n",
        "\n",
        "                # Get predictions\n",
        "                y_prob = model.predict(X_test, batch_size=4096)\n",
        "                y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "                # Calculate AUC\n",
        "                auc_score = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "            else:\n",
        "                # Scikit-learn based models\n",
        "                # Scikit-learn based models\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Get predictions\n",
        "                y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
        "                y_pred = (y_prob > 0.5).astype(int) if hasattr(y_prob, \"astype\") else y_prob\n",
        "\n",
        "                # Calculate AUC\n",
        "                auc_score = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "            # Calculate metrics\n",
        "            precision = precision_score(y_test, y_pred)\n",
        "            recall = recall_score(y_test, y_pred)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "            model_duration = time.time() - model_start\n",
        "\n",
        "            # Print results\n",
        "            print(f\"{name} Results:\")\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"Precision: {precision:.4f}\")\n",
        "            print(f\"Recall: {recall:.4f}\")\n",
        "            print(f\"F1 Score: {f1:.4f}\")\n",
        "            print(f\"AUC: {auc_score:.4f}\")\n",
        "            print(f\"Training Time: {model_duration:.2f} seconds\\n\")\n",
        "\n",
        "            # Store performance\n",
        "            performance.append({\n",
        "                'Model': name,\n",
        "                'Accuracy': accuracy,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1 Score': f1,\n",
        "                'AUC': auc_score,\n",
        "                'Training Time': model_duration\n",
        "            })\n",
        "\n",
        "            # Store predictions for later visualization\n",
        "            all_predictions[name] = {\n",
        "                'y_prob': y_prob,\n",
        "                'y_pred': y_pred\n",
        "            }\n",
        "\n",
        "            # Update best model based on F1 score\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_model = name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame for easier analysis\n",
        "    performance_df = pd.DataFrame(performance)\n",
        "\n",
        "    # Visualize NN training history if available\n",
        "    if nn_history:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(nn_history['loss'], label='Training Loss')\n",
        "        plt.plot(nn_history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Neural Network Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(nn_history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(nn_history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Neural Network Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('plots/neural_network_training.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # Create interactive performance visualization\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Metrics comparison\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
        "\n",
        "    performance_sorted = performance_df.sort_values('F1 Score', ascending=False)\n",
        "    models_sorted = performance_sorted['Model'].tolist()\n",
        "\n",
        "    colors = [\n",
        "        '#3498db', '#e74c3c', '#2ecc71', '#f39c12',\n",
        "        '#9b59b6', '#1abc9c', '#34495e'\n",
        "    ]\n",
        "\n",
        "    # Plot 1: Model performance metrics\n",
        "    plt.subplot(2, 2, 1)\n",
        "    bar_width = 0.15\n",
        "    x = np.arange(len(models_sorted))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.bar(\n",
        "            x + i * bar_width,\n",
        "            performance_sorted[metric],\n",
        "            width=bar_width,\n",
        "            label=metric,\n",
        "            color=colors[i % len(colors)]\n",
        "        )\n",
        "\n",
        "    plt.xlabel('Models', fontsize=12)\n",
        "    plt.ylabel('Score', fontsize=12)\n",
        "    plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(x + bar_width * 2, models_sorted, rotation=45, ha='right')\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Plot 2: Training time comparison\n",
        "    plt.subplot(2, 2, 2)\n",
        "    bars = plt.bar(\n",
        "        models_sorted,\n",
        "        performance_sorted['Training Time'],\n",
        "        color='#3498db'\n",
        "    )\n",
        "\n",
        "    # Add time labels on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(\n",
        "            bar.get_x() + bar.get_width()/2.,\n",
        "            height + 0.1,\n",
        "            f'{height:.1f}s',\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            fontweight='bold'\n",
        "        )\n",
        "\n",
        "    plt.xlabel('Models', fontsize=12)\n",
        "    plt.ylabel('Time (seconds)', fontsize=12)\n",
        "    plt.title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Plot 3: ROC curves\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for i, model_name in enumerate(models_sorted):\n",
        "        if model_name in all_predictions:\n",
        "            y_prob = all_predictions[model_name]['y_prob']\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "            auc_val = auc(fpr, tpr)\n",
        "\n",
        "            plt.plot(\n",
        "                fpr,\n",
        "                tpr,\n",
        "                label=f'{model_name} (AUC = {auc_val:.4f})',\n",
        "                color=colors[i % len(colors)],\n",
        "                linewidth=2\n",
        "            )\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Create confusion matrices for each model\n",
        "    rows = int(np.ceil(len(models_sorted) / 3))\n",
        "    plt.figure(figsize=(18, rows * 5))\n",
        "\n",
        "    for i, model_name in enumerate(models_sorted):\n",
        "        if model_name in all_predictions:\n",
        "            y_pred = all_predictions[model_name]['y_pred']\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            plt.subplot(rows, 3, i+1)\n",
        "            sns.heatmap(\n",
        "                cm,\n",
        "                annot=True,\n",
        "                fmt=\"d\",\n",
        "                cmap=fraud_cmap,\n",
        "                cbar=False\n",
        "            )\n",
        "\n",
        "            plt.title(f'{model_name} Confusion Matrix', fontsize=14)\n",
        "            plt.xlabel('Predicted Label', fontsize=10)\n",
        "            plt.ylabel('True Label', fontsize=10)\n",
        "            plt.xticks([0.5, 1.5], ['Normal', 'Fraud'], rotation=0)\n",
        "            plt.yticks([0.5, 1.5], ['Normal', 'Fraud'], rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Visualize the precision-recall curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for i, model_name in enumerate(models_sorted):\n",
        "        if model_name in all_predictions:\n",
        "            y_prob = all_predictions[model_name]['y_prob']\n",
        "            precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "            plt.plot(\n",
        "                recall_curve,\n",
        "                precision_curve,\n",
        "                label=f'{model_name} (F1 = {performance_sorted[performance_sorted[\"Model\"] == model_name][\"F1 Score\"].values[0]:.4f})',\n",
        "                color=colors[i % len(colors)],\n",
        "                linewidth=2\n",
        "            )\n",
        "\n",
        "    # Add a horizontal line representing the baseline\n",
        "    plt.axhline(y=sum(y_test)/len(y_test), color='k', linestyle='--',\n",
        "               label=f'No Skill (Baseline = {sum(y_test)/len(y_test):.4f})')\n",
        "\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Return best model and performance data\n",
        "    return best_model, performance_df, all_predictions\n",
        "\n",
        "# Train and evaluate models\n",
        "best_model, performance_df, all_predictions = train_and_evaluate_models(\n",
        "    X_train_resampled, y_train_resampled, X_test, y_test\n",
        ")\n",
        "\n",
        "print(f\"\\nüèÜ Best performing model: {best_model}\")\n",
        "print(performance_df.sort_values('F1 Score', ascending=False).to_string(index=False))\n",
        "\n",
        "# Step 5: Feature Importance Analysis for the best model\n",
        "@time_operation(\"Feature Importance Analysis\")\n",
        "def feature_importance_analysis():\n",
        "    # Select the best model from the trained models\n",
        "    if best_model not in models:\n",
        "        print(\"Best model not found in the trained models.\")\n",
        "        return\n",
        "\n",
        "    model = models[best_model]\n",
        "    feature_names = df.drop(columns=['Class']).columns.tolist()\n",
        "\n",
        "    # Different feature importance extraction depending on model type\n",
        "    if best_model == \"Neural Network\":\n",
        "        # For Neural Network, we use permutation importance\n",
        "        from sklearn.inspection import permutation_importance\n",
        "\n",
        "        result = permutation_importance(\n",
        "            model, X_test, y_test,\n",
        "            n_repeats=5, random_state=42, n_jobs=-1\n",
        "        )\n",
        "\n",
        "        importances = result.importances_mean\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    elif best_model in [\"XGBoost\", \"Random Forest\", \"Gradient Boosting\", \"Decision Tree\"]:\n",
        "        # Tree-based models\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "        else:\n",
        "            print(f\"Feature importance not available for {best_model}\")\n",
        "            return\n",
        "\n",
        "    elif best_model == \"Logistic Regression\":\n",
        "        # Logistic Regression\n",
        "        if hasattr(model, 'coef_'):\n",
        "            importances = np.abs(model.coef_[0])\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "        else:\n",
        "            print(f\"Feature importance not available for {best_model}\")\n",
        "            return\n",
        "\n",
        "    else:\n",
        "        # For other models, try to use permutation importance\n",
        "        try:\n",
        "            from sklearn.inspection import permutation_importance\n",
        "\n",
        "            result = permutation_importance(\n",
        "                model, X_test, y_test,\n",
        "                n_repeats=5, random_state=42, n_jobs=-1\n",
        "            )\n",
        "\n",
        "            importances = result.importances_mean\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "        except:\n",
        "            print(f\"Feature importance not available for {best_model}\")\n",
        "            return\n",
        "\n",
        "    # Display top 20 features\n",
        "    top_n = min(20, len(feature_names))\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Get the most important features\n",
        "    top_indices = indices[:top_n]\n",
        "    top_importances = importances[top_indices]\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "\n",
        "    # Create a DataFrame for easier sorting\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': top_features,\n",
        "        'Importance': top_importances\n",
        "    })\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=True)\n",
        "\n",
        "    # Plot horizontal bar chart\n",
        "    bars = plt.barh(importance_df['Feature'], importance_df['Importance'], color='#3498db')\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        plt.text(\n",
        "            bar.get_width() * 1.01,\n",
        "            bar.get_y() + bar.get_height()/2,\n",
        "            f\"{importance_df['Importance'].iloc[i]:.4f}\",\n",
        "            va='center',\n",
        "            fontweight='bold'\n",
        "        )\n",
        "\n",
        "    plt.xlabel('Importance', fontsize=12)\n",
        "    plt.ylabel('Feature', fontsize=12)\n",
        "    plt.title(f'Top {top_n} Features for {best_model}', fontsize=16, fontweight='bold')\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig('plots/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Analyze feature importance\n",
        "importance_df = feature_importance_analysis()\n",
        "\n",
        "if importance_df is not None:\n",
        "    print(\"\\nTop 10 most important features:\")\n",
        "    print(importance_df.sort_values('Importance', ascending=False).head(10))\n",
        "\n",
        "# Step 6: Real-time Fraud Detection Simulation\n",
        "@time_operation(\"Fraud Detection Simulation\")\n",
        "def simulate_fraud_detection():\n",
        "    print(\"Simulating real-time fraud detection pipeline...\")\n",
        "\n",
        "    if best_model not in models:\n",
        "        print(\"Best model not found in the trained models.\")\n",
        "        return\n",
        "\n",
        "    model = models[best_model]\n",
        "    feature_names = df.drop(columns=['Class']).columns.tolist()\n",
        "\n",
        "    # Create test cases - a mix of normal and fraud transactions\n",
        "    # Sample a few random examples from the test set\n",
        "    normal_indices = np.where(y_test == 0)[0]\n",
        "    fraud_indices = np.where(y_test == 1)[0]\n",
        "\n",
        "    # Select random examples for the simulation\n",
        "    n_normal = min(5, len(normal_indices))\n",
        "    n_fraud = min(5, len(fraud_indices))\n",
        "\n",
        "    selected_normal = np.random.choice(normal_indices, n_normal, replace=False)\n",
        "    selected_fraud = np.random.choice(fraud_indices, n_fraud, replace=False)\n",
        "\n",
        "    # Combine the indices\n",
        "    selected_indices = np.concatenate([selected_normal, selected_fraud])\n",
        "    np.random.shuffle(selected_indices)\n",
        "\n",
        "    # Get the selected examples\n",
        "    X_sample = X_test[selected_indices]\n",
        "    y_sample = y_test[selected_indices]\n",
        "\n",
        "    # Create a figure to save all simulations\n",
        "    plt.figure(figsize=(14, len(selected_indices) * 3))\n",
        "\n",
        "    # Perform real-time detection\n",
        "    detection_results = []\n",
        "\n",
        "    for i, (x, y_true) in enumerate(zip(X_sample, y_sample)):\n",
        "        # Reshape for single prediction\n",
        "        x_reshape = x.reshape(1, -1)\n",
        "\n",
        "        # Measure prediction time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Get prediction\n",
        "        if best_model == \"Neural Network\":\n",
        "            y_prob = model.predict(x_reshape)[0][0]\n",
        "        else:\n",
        "            y_prob = model.predict_proba(x_reshape)[0][1] if hasattr(model, \"predict_proba\") else model.predict(x_reshape)[0]\n",
        "\n",
        "        y_pred = 1 if y_prob > 0.5 else 0\n",
        "        end_time = time.time()\n",
        "\n",
        "        detection_time = (end_time - start_time) * 1000  # in milliseconds\n",
        "\n",
        "        # Create a subplot for each transaction\n",
        "        plt.subplot(len(selected_indices), 1, i + 1)\n",
        "        bar_colors = ['#3498db', '#e74c3c']\n",
        "\n",
        "        # Create a horizontal bar for confidence\n",
        "        bars = plt.barh(\n",
        "            ['Normal', 'Fraud'],\n",
        "            [1 - y_prob, y_prob],\n",
        "            color=[bar_colors[0], bar_colors[1]],\n",
        "            height=0.5\n",
        "        )\n",
        "\n",
        "        # Add percentage text\n",
        "        for bar, val in zip(bars, [1 - y_prob, y_prob]):\n",
        "            plt.text(\n",
        "                min(val + 0.02, 0.98) if val > 0.92 else val + 0.02,\n",
        "                bar.get_y() + bar.get_height()/2,\n",
        "                f\"{val:.2%}\",\n",
        "                va='center',\n",
        "                fontweight='bold',\n",
        "                color='white' if val > 0.3 else 'black'\n",
        "            )\n",
        "\n",
        "        # Highlight the correct class\n",
        "        correct = y_pred == y_true\n",
        "        result_color = '#2ecc71' if correct else '#e74c3c'\n",
        "        result_text = \"Correct\" if correct else \"Incorrect\"\n",
        "\n",
        "        plt.title(\n",
        "            f\"Transaction {i+1}: {'Fraud' if y_true == 1 else 'Normal'} | \"\n",
        "            f\"Prediction: {'Fraud' if y_pred == 1 else 'Normal'} | \"\n",
        "            f\"Result: {result_text} | \"\n",
        "            f\"Detection time: {detection_time:.2f}ms\",\n",
        "            fontweight='bold',\n",
        "            color=result_color\n",
        "        )\n",
        "\n",
        "        plt.xlim(0, 1)\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "        # Store result for the final report\n",
        "        detection_results.append({\n",
        "            'Transaction': i + 1,\n",
        "            'True Class': 'Fraud' if y_true == 1 else 'Normal',\n",
        "            'Predicted Class': 'Fraud' if y_pred == 1 else 'Normal',\n",
        "            'Fraud Probability': y_prob,\n",
        "            'Correct': correct,\n",
        "            'Detection Time (ms)': detection_time\n",
        "        })\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/fraud_detection_simulation.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Create a summary of the simulation\n",
        "    detection_df = pd.DataFrame(detection_results)\n",
        "\n",
        "    # Calculate accuracy and average detection time\n",
        "    accuracy = detection_df['Correct'].mean()\n",
        "    avg_detection_time = detection_df['Detection Time (ms)'].mean()\n",
        "\n",
        "    print(f\"Simulation Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(f\"Average Detection Time: {avg_detection_time:.2f} ms\")\n",
        "\n",
        "    return detection_df\n",
        "\n",
        "# Run fraud detection simulation\n",
        "detection_results = simulate_fraud_detection()\n",
        "\n",
        "# Final summary and report\n",
        "@time_operation(\"Performance Summary\")\n",
        "def create_performance_summary():\n",
        "    # End time for total script run\n",
        "    end_time_total = time.time()\n",
        "    total_duration = end_time_total - start_time_total\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CREDIT CARD FRAUD DETECTION - PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nTotal Processing Time: {total_duration:.2f} seconds\")\n",
        "\n",
        "    # Dataset summary\n",
        "    print(\"\\nDataset Summary:\")\n",
        "    print(f\"- Total transactions: {len(df)}\")\n",
        "    print(f\"- Fraud transactions: {int(df['Class'].sum())} ({eda_stats['fraud_rate']:.4f}%)\")\n",
        "    print(f\"- Normal transactions: {len(df) - int(df['Class'].sum())}\")\n",
        "\n",
        "    # Display performance timing\n",
        "    print(\"\\nPipeline Performance:\")\n",
        "    for operation, duration in perf_metrics.items():\n",
        "        print(f\"- {operation}: {duration:.2f} seconds\")\n",
        "\n",
        "    # Best model details\n",
        "    print(f\"\\nBest Model: {best_model}\")\n",
        "    best_performance = performance_df[performance_df['Model'] == best_model].iloc[0]\n",
        "    print(f\"- Accuracy: {best_performance['Accuracy']:.4f}\")\n",
        "    print(f\"- Precision: {best_performance['Precision']:.4f}\")\n",
        "    print(f\"- Recall: {best_performance['Recall']:.4f}\")\n",
        "    print(f\"- F1 Score: {best_performance['F1 Score']:.4f}\")\n",
        "    print(f\"- AUC: {best_performance['AUC']:.4f}\")\n",
        "\n",
        "    # Top features\n",
        "    if importance_df is not None:\n",
        "        print(\"\\nTop 5 Most Important Features:\")\n",
        "        for i, row in importance_df.sort_values('Importance', ascending=False).head(5).iterrows():\n",
        "            print(f\"- {row['Feature']}: {row['Importance']:.4f}\")\n",
        "\n",
        "    # Save summary to file\n",
        "    with open('performance_summary.txt', 'w') as f:\n",
        "        f.write(\"CREDIT CARD FRAUD DETECTION - PERFORMANCE SUMMARY\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Total Processing Time: {total_duration:.2f} seconds\\n\\n\")\n",
        "\n",
        "        f.write(\"Dataset Summary:\\n\")\n",
        "        f.write(f\"- Total transactions: {len(df)}\\n\")\n",
        "        f.write(f\"- Fraud transactions: {int(df['Class'].sum())} ({eda_stats['fraud_rate']:.4f}%)\\n\")\n",
        "        f.write(f\"- Normal transactions: {len(df) - int(df['Class'].sum())}\\n\\n\")\n",
        "\n",
        "        f.write(\"Pipeline Performance:\\n\")\n",
        "        for operation, duration in perf_metrics.items():\n",
        "            f.write(f\"- {operation}: {duration:.2f} seconds\\n\")\n",
        "\n",
        "        f.write(f\"\\nBest Model: {best_model}\\n\")\n",
        "        f.write(f\"- Accuracy: {best_performance['Accuracy']:.4f}\\n\")\n",
        "        f.write(f\"- Precision: {best_performance['Precision']:.4f}\\n\")\n",
        "        f.write(f\"- Recall: {best_performance['Recall']:.4f}\\n\")\n",
        "        f.write(f\"- F1 Score: {best_performance['F1 Score']:.4f}\\n\")\n",
        "        f.write(f\"- AUC: {best_performance['AUC']:.4f}\\n\")\n",
        "\n",
        "        if importance_df is not None:\n",
        "            f.write(\"\\nTop Features:\\n\")\n",
        "            for i, row in importance_df.sort_values('Importance', ascending=False).head(10).iterrows():\n",
        "                f.write(f\"- {row['Feature']}: {row['Importance']:.4f}\\n\")\n",
        "\n",
        "    print(\"\\nPerformance summary saved to performance_summary.txt\")\n",
        "    print(\"\\nAll visualizations saved to the 'plots' directory.\")\n",
        "\n",
        "# Generate final performance summary\n",
        "create_performance_summary()\n",
        "\n",
        "print(\"\\n‚úÖ Credit card fraud detection pipeline complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QEZ34DZTPNs8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}